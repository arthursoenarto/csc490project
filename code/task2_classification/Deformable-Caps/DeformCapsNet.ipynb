{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This code is borrowed from: https://github.com/acburigo/CapsNet**\n",
    "\n",
    "**The origional code was developed for another dataset and it was change to accommodate our dataset**\n",
    "\n",
    "**The Oriogonal model is a regular capsule network and it is modified to with deformable convolution operator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a [pytorch](http://pytorch.org/) implementation of CapsNet, described in the paper [Dynamic Routing Between Capsules](https://arxiv.org/abs/1710.09829) - by [Sara Sabour](https://arxiv.org/find/cs/1/au:+Sabour_S/0/1/0/all/0/1), [Nicholas Frosst](https://arxiv.org/find/cs/1/au:+Frosst_N/0/1/0/all/0/1) and [Geoffrey E Hinton](https://arxiv.org/find/cs/1/au:+Hinton_G/0/1/0/all/0/1).\n",
    "\n",
    "All images and text in the following sections are extracted directly from the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is performed on 28 x 28 MNIST images that have been shifted by up to 2 pixels in each direction with zero padding. No other data augmentation/deformation is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_size contains a series of options for image size\n",
    "new_size = [20,50,28,100,150,250,310,450]\n",
    "# Change the size_choice to have a different size of images to train\n",
    "size_choice = 3\n",
    "\n",
    "INPUT_SIZE = (1, new_size[size_choice], new_size[size_choice])\n",
    "\n",
    "# Define the transformation for the data\n",
    "data_transform = torchvision.transforms.Compose(\n",
    "        [   \n",
    "            torchvision.transforms.Grayscale(1),\n",
    "            torchvision.transforms.RandomCrop(INPUT_SIZE[1:]),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ],\n",
    "    )\n",
    "\n",
    "validation_transform = torchvision.transforms.Compose(\n",
    "        [   \n",
    "            torchvision.transforms.Grayscale(1),\n",
    "            torchvision.transforms.RandomCrop(INPUT_SIZE[1:]),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from Dataloaders.py\n",
    "from Datasets import Classification_Datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Path to the data\n",
    "training_data_path = \"./data/classification_dataset/training_images/\"\n",
    "training_label_path = \"./data/classification_dataset/downsampled_training_labels.csv\"\n",
    "validation_data_path = \"./data/classification_dataset/validation_images/\"\n",
    "validation_label_path = \"./data/classification_dataset/validation_labels.csv\"\n",
    "\n",
    "# Creating the datasets\n",
    "training_dataset = Classification_Datasets(training_data_path, training_label_path, data_transform)\n",
    "\n",
    "# Resize the validation images\n",
    "validation_dataset = Classification_Datasets(validation_data_path, validation_label_path, validation_transform)\n",
    "\n",
    "# Set the batch_size and creating data loaders\n",
    "BATCH_SIZE = 1\n",
    "trn_loader = DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "tst_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "def show_batch(dl):\n",
    "    for img, lb in dl:\n",
    "        # print(img.shape)\n",
    "\n",
    "\n",
    "        # print(lb)\n",
    "        # lb = torch.max(lb, 1)\n",
    "        # print(lb[1].shape)\n",
    "        fig, ax = plt.subplots(figsize=(16, 8))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(img.cpu(), nrow=16).permute(1,2,0))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_batch(trn_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 60K and 10K images for training and testing respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define CapsNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv1 has 256, 9 x 9 convolution kernels with a stride of 1 and ReLU activation. This layer converts pixel intensities to the activities of local feature detectors that are then used as inputs to the *primary* capsules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=256, kernel_size=9):\n",
    "        super(Conv1, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = self.conv(x)\n",
    "        x = self.conv = torchvision.ops.deform_conv2d(x, offset=torch.rand(1, 16928, 92,92), weight=torch.rand(1, 5*5, 92,92))\n",
    "        print(x.shape)\n",
    "        x = self.activation(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary Capsules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second layer (PrimaryCapsules) is a convolutional capsule layer with 32 channels of convolutional 8D capsules (*i.e.* each primary capsule contains 8 convolutional units with a $[9 \\times 9]$ kernel and a stride of 2). Each primary capsule output sees the outputs of all $[256 \\times 81]$ Conv1 units whose receptive fields overlap with the location of the center of the capsule. In total PrimaryCapsules has $[32 \\times 6 \\times 6]$ capsule outputs (each output is an 8D vector) and each capsule in the $[6 \\times 6]$ grid is sharing their weights with each other. One can see PrimaryCapsules as a Convolution layer with Eq. 1 as its block non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCapsules(torch.nn.Module):\n",
    "    def __init__(self, input_shape=(256, 20, 20), capsule_dim=8,\n",
    "                 out_channels=32, kernel_size=9, stride=2):\n",
    "        super(PrimaryCapsules, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.capsule_dim = capsule_dim\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.in_channels = self.input_shape[0]\n",
    "        \n",
    "        self.conv = torch.nn.Conv2d(\n",
    "            self.in_channels,\n",
    "            self.out_channels * self.capsule_dim,\n",
    "            self.kernel_size,\n",
    "            self.stride\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "        x = x.view(-1, x.size()[1], x.size()[2], self.out_channels, self.capsule_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the length of the output vector of a capsule to represent the probability that the entity represented by the capsule is present in the current input. We therefore use a non-linear \"squashing\" function to ensure that short vectors get shrunk to almost zero length and long vectors get shrunk to a length slightly below 1. We leave it to discriminative learning to make good use of this non-linearity.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{v}_j = \\frac{||\\mathbf{s}_j||^2}{1 + ||\\mathbf{s}_j||^2} \\frac{\\mathbf{s}_j}{||\\mathbf{s}_j||}\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\mathbf{v}_j$ is the vector output of capsule $j$ and $\\mathbf{s}_j$ is its total input.\n",
    "\n",
    "For all but the first layer of capsules, the total input to a capsule $\\mathbf{s}_j$ is a weighted sum over all \"prediction vectors\" $\\mathbf{\\hat u}_{j|i}$ from the capsules in the layer below and is produced by multiplying the output $\\mathbf{u}_i$ of a capsule in the layer below by a weight matrix $\\mathbf{W}_{ij}$\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{s}_j = \\sum_i c_{ij} \\mathbf{\\hat u}_{j|i}, \\quad \\mathbf{\\hat u}_{j|i} = \\mathbf{W}_{ij} \\mathbf{u}_i\n",
    "\\end{equation*}\n",
    "\n",
    "where the $c_{ij}$ are coupling coefficients that are determined by the iterative dynamic routing process.\n",
    "\n",
    "The coupling coefficients between capsule $i$ and all the capsules in the layer above sum to 1 and are determined by a \"routing softmax\" whose initial logits $b_{ij}$ are the log prior probabilities that capsule $i$ should be coupled to capsule $j$.\n",
    "\n",
    "\\begin{equation*}\n",
    "c_{ij} = \\frac{\\exp(b_{ij})}{\\sum_k \\exp(b_{ik})}\n",
    "\\end{equation*}\n",
    "\n",
    "The log priors can be learned discriminatively at the same time as all the other weights. They depend on the location and type of the two capsules but not on the current input image. The initial coupling coefficients are then iteratively refined by measuring the agreement between the current output $\\mathbf{v}_j$ of each capsule, $j$, in the layer above and the prediction $\\mathbf{\\hat u}_{j|i}$ made by capsule $i$.\n",
    "\n",
    "The agreement is simply the scalar product $a_{ij} = \\mathbf{v}_j \\cdot \\mathbf{\\hat u}_{j|i}$. This agreement is treated as if it was a log likelihood and is added to the initial logit, $b_{ij}$ before computing the new values for all the coupling coefficients linking capsule $i$ to higher level capsules.\n",
    "\n",
    "In convolutional capsule layers, each capsule outputs a local grid of vectors to each type of capsule in the layer above using different transformation matrices for each member of the grid as well as for each type of capsule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Routing(torch.nn.Module):\n",
    "    def __init__(self, caps_dim_before=8, caps_dim_after=16,\n",
    "                 n_capsules_before=(6 * 6 * 32), n_capsules_after=10):\n",
    "        super(Routing, self).__init__()\n",
    "        self.n_capsules_before = n_capsules_before\n",
    "        self.n_capsules_after = n_capsules_after\n",
    "        self.caps_dim_before = caps_dim_before\n",
    "        self.caps_dim_after = caps_dim_after\n",
    "        \n",
    "        # Parameter initialization not specified in the paper\n",
    "        n_in = self.n_capsules_before * self.caps_dim_before\n",
    "        variance = 2 / (n_in)\n",
    "        std = np.sqrt(variance)\n",
    "        self.W = torch.nn.Parameter(\n",
    "            torch.randn(\n",
    "                self.n_capsules_before,\n",
    "                self.n_capsules_after,\n",
    "                self.caps_dim_after,\n",
    "                self.caps_dim_before) * std,\n",
    "            requires_grad=True)\n",
    "    \n",
    "    # Equation (1)\n",
    "    @staticmethod\n",
    "    def squash(s):\n",
    "        s_norm = torch.norm(s, p=2, dim=-1, keepdim=True)\n",
    "        s_norm2 = torch.pow(s_norm, 2)\n",
    "        v = (s_norm2 / (1.0 + s_norm2)) * (s / s_norm)\n",
    "        return v\n",
    "    \n",
    "    # Equation (2)\n",
    "    def affine(self, x):\n",
    "        x = self.W @ x.unsqueeze(2).expand(-1, -1, 10, -1).unsqueeze(-1)\n",
    "        return x.squeeze()\n",
    "    \n",
    "    # Equation (3)\n",
    "    @staticmethod\n",
    "    def softmax(x, dim=-1):\n",
    "        exp = torch.exp(x)\n",
    "        return exp / torch.sum(exp, dim, keepdim=True)\n",
    "    \n",
    "    # Procedure 1 - Routing algorithm.\n",
    "    def routing(self, u, r, l):\n",
    "        b = Variable(torch.zeros(u.size()[0], l[0], l[1]), requires_grad=False).cuda() # torch.Size([?, 1152, 10])\n",
    "        \n",
    "        for iteration in range(r):\n",
    "            c = Routing.softmax(b) # torch.Size([?, 1152, 10])\n",
    "            s = (c.unsqueeze(-1).expand(-1, -1, -1, u.size()[-1]) * u).sum(1) # torch.Size([?, 1152, 16])\n",
    "            v = Routing.squash(s) # torch.Size([?, 10, 16])\n",
    "            b += (u * v.unsqueeze(1).expand(-1, l[0], -1, -1)).sum(-1)\n",
    "        return v\n",
    "    \n",
    "    def forward(self, x, n_routing_iter):\n",
    "        x = x.view((-1, self.n_capsules_before, self.caps_dim_before))\n",
    "        x = self.affine(x) # torch.Size([?, 1152, 10, 16])\n",
    "        x = self.routing(x, n_routing_iter, (self.n_capsules_before, self.n_capsules_after))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final Layer (DigitCaps) has one 16D capsule per digit class and each of these capsules receives input from all the capsules in the layer below.\n",
    "\n",
    "We have routing only between two consecutive capsule layers (e.g. PrimaryCapsules and DigitCaps).\n",
    "Since Conv1 output is 1D, there is no orientation in its space to agree on. Therefore, no routing is used between Conv1 and PrimaryCapsules. All the routing logits ($b_{ij}$) are initialized to zero. Therefore, initially a capsule output ($\\mathbf{u}_i$) is sent to all parent capsules ($\\mathbf{v}_0...\\mathbf{v}_9$) with equal probability ($c_{ij}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the length of the instantiation vector to represent the probability that a capsuleâ€™s entity exists. We would like the top-level capsule for digit class $k$ to have a long instantiation vector if and only if that digit is present in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Norm, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.norm(x, p=2, dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, output_size=INPUT_SIZE):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = self.assemble_decoder(in_features, out_features)\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    def assemble_decoder(self, in_features, out_features):\n",
    "        HIDDEN_LAYER_FEATURES = [512, 1024]\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features, HIDDEN_LAYER_FEATURES[0]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(HIDDEN_LAYER_FEATURES[0], HIDDEN_LAYER_FEATURES[1]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(HIDDEN_LAYER_FEATURES[1], out_features),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        x = x[np.arange(0, x.size()[0]), y.cpu().data.numpy(), :].cuda()\n",
    "        x = self.decoder(x)\n",
    "        x = x.view(*((-1,) + self.output_size))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CapsNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsNet(torch.nn.Module):\n",
    "    def __init__(self, input_shape=INPUT_SIZE, n_routing_iter=3, use_reconstruction=True):\n",
    "        super(CapsNet, self).__init__()\n",
    "        assert len(input_shape) == 3\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.n_routing_iter = n_routing_iter\n",
    "        self.use_reconstruction = use_reconstruction\n",
    "        \n",
    "        self.conv1 = Conv1(input_shape[0], 256, 9)\n",
    "        self.primary_capsules = PrimaryCapsules(\n",
    "            input_shape=(256, 20, 20),\n",
    "            capsule_dim=8,\n",
    "            out_channels=32,\n",
    "            kernel_size=9,\n",
    "            stride=2\n",
    "        )\n",
    "        self.routing = Routing(\n",
    "            caps_dim_before=8,\n",
    "            caps_dim_after=16,\n",
    "            n_capsules_before=6 * 6 * 32,\n",
    "            n_capsules_after=10\n",
    "        )\n",
    "        self.norm = Norm()\n",
    "        \n",
    "        if (self.use_reconstruction):\n",
    "            self.decoder = Decoder(16, int(np.prod(input_shape)))\n",
    "    \n",
    "    def n_parameters(self):\n",
    "        return np.sum([np.prod(x.size()) for x in self.parameters()])\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        conv1 = self.conv1(x)\n",
    "        primary_capsules = self.primary_capsules(conv1)\n",
    "        digit_caps = self.routing(primary_capsules, self.n_routing_iter)\n",
    "        scores = self.norm(digit_caps)\n",
    "        \n",
    "        if (self.use_reconstruction and y is not None):\n",
    "            reconstruction = self.decoder(digit_caps, y).view((-1,) + self.input_shape)\n",
    "            return scores, reconstruction\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Margin Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow for multiple digits, we use a separate margin loss, $L_k$ for each digit capsule, $k$:\n",
    "\n",
    "\\begin{equation*}\n",
    "L_k = T_k \\max(0, m^+ - ||\\mathbf{v}_k||)^2 + \\lambda (1 - T_k) \\max(0, ||\\mathbf{v}_k|| - m^-)^2\n",
    "\\end{equation*}\n",
    "\n",
    "where $T_k = 1$ iff a digit of class $k$ is present and $m^+ = 0.9$ and $m^- = 0.1$. The $\\lambda$ down-weighting of the loss for absent digit classes stops the initial learning from shrinking the lengths of the activity vectors of all the digit capsules. We use $\\lambda = 0.5$. The total loss is simply the sum of the losses of all digit capsules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    new_y = torch.eye(num_classes)[y.cpu().data.numpy(),]\n",
    "    if (y.is_cuda):\n",
    "        return new_y.cuda()\n",
    "    return new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarginLoss(torch.nn.Module):\n",
    "    def __init__(self, m_pos=0.9, m_neg=0.1, lamb=0.5):\n",
    "        super(MarginLoss, self).__init__()\n",
    "        self.m_pos = m_pos\n",
    "        self.m_neg = m_neg\n",
    "        self.lamb = lamb\n",
    "    \n",
    "    # Equation (4)\n",
    "    def forward(self, scores, y):\n",
    "        y = Variable(to_categorical(y, 10))\n",
    "        \n",
    "        Tc = y.float()\n",
    "        loss_pos = torch.pow(torch.clamp(self.m_pos - scores, min=0), 2)\n",
    "        loss_neg = torch.pow(torch.clamp(scores - self.m_neg, min=0), 2)\n",
    "        loss = Tc * loss_pos + self.lamb * (1 - Tc) * loss_neg\n",
    "        loss = loss.sum(-1)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use an additional reconstruction loss to encourage the digit capsules to encode the instantiation parameters of the input digit. (...) We minimize the sum of squared differences between the outputs of the logistic units and the pixel intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumSquaredDifferencesLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SumSquaredDifferencesLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, x_reconstruction, x):\n",
    "        loss = torch.pow(x - x_reconstruction, 2).sum(-1).sum(-1)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scale down this reconstruction loss by $0.0005$ so that it does not dominate the margin loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsNetLoss(torch.nn.Module):\n",
    "    def __init__(self, reconstruction_loss_scale=0.0005):\n",
    "        super(CapsNetLoss, self).__init__()\n",
    "        self.digit_existance_criterion = MarginLoss()\n",
    "        self.digit_reconstruction_criterion = SumSquaredDifferencesLoss()\n",
    "        self.reconstruction_loss_scale = reconstruction_loss_scale\n",
    "    \n",
    "    def forward(self, x, y, x_reconstruction, scores):\n",
    "        margin_loss = self.digit_existance_criterion(y_pred.cuda(), y)\n",
    "        reconstruction_loss = self.reconstruction_loss_scale *\\\n",
    "                              self.digit_reconstruction_criterion(x_reconstruction, x)\n",
    "        loss = margin_loss + reconstruction_loss\n",
    "        return loss, margin_loss, reconstruction_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CapsNet(\n",
       "  (conv1): Conv1(\n",
       "    (conv): Conv2d(1, 256, kernel_size=(9, 9), stride=(1, 1))\n",
       "    (activation): ReLU()\n",
       "  )\n",
       "  (primary_capsules): PrimaryCapsules(\n",
       "    (conv): Conv2d(256, 256, kernel_size=(9, 9), stride=(2, 2))\n",
       "  )\n",
       "  (routing): Routing()\n",
       "  (norm): Norm()\n",
       "  (decoder): Decoder(\n",
       "    (decoder): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=1024, out_features=10000, bias=True)\n",
       "      (5): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CapsNet().cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CapsNet has 8.2M parameters and 6.8M parameters without the reconstruction subnetwork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters: 17588240\n"
     ]
    }
   ],
   "source": [
    "print('Number of Parameters: %d' % model.n_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CapsNetLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(...) we use the Adam optimizer with its TensorFlow default parameters, including the exponentially decaying learning rate, to minimize the sum of the margin losses in Eq. 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(optimizer, learning_rate, global_step, decay_steps, decay_rate, staircase=False):\n",
    "    if (staircase):\n",
    "        decayed_learning_rate = learning_rate * np.power(decay_rate, global_step // decay_steps)\n",
    "    else:\n",
    "        decayed_learning_rate = learning_rate * np.power(decay_rate, global_step / decay_steps)\n",
    "        \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = decayed_learning_rate\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-08\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, train_accuracy, test_accuracy, model, optimizer, path=None):\n",
    "    if (path is None):\n",
    "        path = 'checkpoint-%f-%04d.pth' % (test_accuracy, epoch)\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(state, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example(model, x, y, x_reconstruction, y_pred):\n",
    "    x = x.squeeze().cpu().data.numpy()\n",
    "    y = y.cpu().data.numpy()\n",
    "    x_reconstruction = x_reconstruction.squeeze().cpu().data.numpy()\n",
    "    _, y_pred = torch.max(y_pred, -1)\n",
    "    y_pred = y_pred.cpu().data.numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    ax[0].imshow(x, cmap='Greys')\n",
    "    ax[0].set_title('Input: %d' % y)\n",
    "    ax[1].imshow(x_reconstruction, cmap='Greys')\n",
    "    ax[1].set_title('Output: %d' % y_pred)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader):\n",
    "    metrics = defaultdict(lambda:list())\n",
    "    for batch_id, (x, y) in tqdm(enumerate(loader), total=len(loader)):\n",
    "        x = Variable(x).float().cuda()\n",
    "        y = torch.max(Variable(y).cuda(), 1)[1]\n",
    "        y_pred, x_reconstruction = model(x, y)\n",
    "        _, y_pred = torch.max(y_pred, -1)\n",
    "        metrics['accuracy'].append((y_pred == y).cpu().data.numpy())\n",
    "    metrics['accuracy'] = np.concatenate(metrics['accuracy']).mean()\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_epoch = 0\n",
    "global_step = 0\n",
    "best_tst_accuracy = 0.0\n",
    "history = defaultdict(lambda:list())\n",
    "COMPUTE_TRN_METRICS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch %d (%d/%d):' % (global_epoch + 1, epoch + 1, n_epochs))\n",
    "    \n",
    "    for batch_id, (x, y) in tqdm(enumerate(trn_loader), total=len(trn_loader)):\n",
    "        optimizer = exponential_decay(optimizer, LEARNING_RATE, global_epoch, 1, 0.90) # Configurations not specified in the paper\n",
    "        \n",
    "        x = Variable(x).float().to(DEVICE)\n",
    "        y = torch.max(Variable(y).cuda(), 1)[1]\n",
    "        \n",
    "        y_pred, x_reconstruction = model(x, y)\n",
    "        loss, margin_loss, reconstruction_loss = criterion(x, y, x_reconstruction, y_pred.cuda())\n",
    "        \n",
    "        history['margin_loss'].append(margin_loss.cpu().data.numpy())\n",
    "        history['reconstruction_loss'].append(reconstruction_loss.cpu().data.numpy())\n",
    "        history['loss'].append(loss.cpu().data.numpy())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        global_step += 1\n",
    "\n",
    "    trn_metrics = test(model, trn_loader) if COMPUTE_TRN_METRICS else None\n",
    "    tst_metrics = test(model, tst_loader)\n",
    "    \n",
    "    print('Example:')\n",
    "    idx = np.random.randint(0, len(x))\n",
    "    show_example(model, x[idx], y[idx], x_reconstruction[idx], y_pred[idx])\n",
    "    \n",
    "    if (tst_metrics['accuracy'] >= best_tst_accuracy):\n",
    "        best_tst_accuracy = tst_metrics['accuracy']\n",
    "        save_checkpoint(\n",
    "            global_epoch + 1,\n",
    "            trn_metrics['accuracy'] if COMPUTE_TRN_METRICS else 0.0,\n",
    "            tst_metrics['accuracy'],\n",
    "            model,\n",
    "            optimizer\n",
    "        )\n",
    "    global_epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_curve(y, n_points_avg):\n",
    "    avg_kernel = np.ones((n_points_avg,)) / n_points_avg\n",
    "    rolling_mean = np.convolve(y, avg_kernel, mode='valid')\n",
    "    return rolling_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points_avg = 10\n",
    "n_points_plot = 1000\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "curve = np.asarray(history['loss'])[-n_points_plot:]\n",
    "avg_curve = compute_avg_curve(curve, n_points_avg)\n",
    "plt.plot(avg_curve, '-g')\n",
    "\n",
    "curve = np.asarray(history['margin_loss'])[-n_points_plot:]\n",
    "avg_curve = compute_avg_curve(curve, n_points_avg)\n",
    "plt.plot(avg_curve, '-b')\n",
    "\n",
    "curve = np.asarray(history['reconstruction_loss'])[-n_points_plot:]\n",
    "avg_curve = compute_avg_curve(curve, n_points_avg)\n",
    "plt.plot(avg_curve, '-r')\n",
    "\n",
    "plt.legend(['Total Loss', 'Margin Loss', 'Reconstruction Loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "nav_menu": {
    "height": "177px",
    "width": "219px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "659px",
    "left": "0px",
    "right": "1007.8px",
    "top": "133px",
    "width": "241px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
